---
title: MÃ´ hÃ¬nh Markov áº©n vÃ  bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n
date: 2022-1-3 20:51:00 +/-0084
categories: [knowledge]
tags: [machine learning, probability, nlp]
toc: true
comments: true
published: true
math: true
---

### Ná»™i dung
- [1. Äá»‹nh nghÄ©a](#-dinh-nghia)
- [2. Ba bÃ i toÃ¡n ná»n táº£ng](#-three-problems)
- [3. BÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n](#-bai-toan-phan-tich-cam-xuc-van-ban)
    - [3.1 Giá»›i thiá»‡u bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n](#-gioi-thieu-bai-toan)
    - [3.2 Bá»™ dá»¯ liá»‡u Financial News cá»§a Kaggle](#-bo-du-lieu)
    - [3.3 MÃ´ hÃ¬nh bÃ i toÃ¡n](#-mo-hinh-bai-toan)
    - [3.4 PhÆ°Æ¡ng phÃ¡p thá»±c hiá»‡n](#-phuong-phap)
- [4. Tá»•ng káº¿t](#-tong-ket)
- [5. Tham kháº£o](#-tham-khao)

<a name="-dinh-nghia"></a>
# 1. Äá»‹nh nghÄ©a

á» bÃ i viáº¿t vá» [Markov chain](/posts/markov-chain-va-bai-toan-sang-nay-an-gi/), chÃºng ta Ä‘Ã£ tÃ¬m hiá»ƒu vá» má»™t mÃ´ hÃ¬nh Ä‘Æ°á»£c káº¿t há»£p bá»Ÿi cÃ¡c tráº¡ng thÃ¡i, cÃ¡c tráº¡ng thÃ¡i cÅ©ng Ä‘á»“ng thá»i cÅ©ng lÃ  káº¿t quáº£ cá»§a mÃ´ hÃ¬nh. Trong bÃ i viáº¿t nÃ y, chÃºng ta sáº½ tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh Markov áº©n (Hidden Markov model - HMM), mÃ  cÃ¡c tráº¡ng thÃ¡i cá»§a mÃ´ hÃ¬nh lÃºc nÃ y sáº½ khÃ´ng pháº£i lÃ  thá»© chÃºng ta cÃ³ thá»ƒ quan sÃ¡t Ä‘Æ°á»£c.

MÃ´ hÃ¬nh Markov áº©n lÃ  má»™t mÃ´ hÃ¬nh thá»‘ng kÃª Ä‘Æ°á»£c káº¿t há»£p bá»Ÿi táº­p cÃ¡c tráº¡ng thÃ¡i áº©n (hidden state) vÃ  táº­p cÃ¡c quan sÃ¡t (observation). MÃ´ hÃ¬nh Markov áº©n sá»­ dá»¥ng tÃ­nh cháº¥t Markov giá»‘ng Markov chain, tráº¡ng thÃ¡i hiá»‡n táº¡i chá»‰ phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³, ngoÃ i ra cÃ¡c quan sÃ¡t hiá»‡n táº¡i chá»‰ phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i hiá»‡n táº¡i.

MÃ´ hÃ¬nh Markov áº©n tá»«ng thá»‘ng trá»‹ ráº¥t nhiá»u bÃ i toÃ¡n vÃ  lÄ©nh vá»±c á»Ÿ tháº­p ká»· trÆ°á»›c (chá»©ng cá»© lÃ  cÃ³ ráº¥t nhiá»u bÃ i bÃ¡o Ä‘Æ°á»£c Ä‘Äƒng táº£i liÃªn quan Ä‘áº¿n mÃ´ hÃ¬nh Markov áº©n liÃªn quan Ä‘áº¿n nhiá»u lÄ©nh vá»±c táº¡i thá»i Ä‘iá»ƒm Ä‘Ã³), nháº¥t lÃ  trong lÄ©nh vá»±c <a href="https://en.wikipedia.org/wiki/Speech_recognition" target="_blank">Nháº­n dáº¡ng giá»ng nÃ³i</a> (Speech Recognition). Trong lÄ©nh vá»±c nháº­n dáº¡ng giá»ng nÃ³i, mÃ´ hÃ¬nh Markov áº©n Ä‘Ã³ng vai trÃ² nhÆ° má»™t mÃ´ hÃ¬nh Ã¢m há»c Ä‘áº¡i diá»‡n cho má»™t Ä‘Æ¡n vá»‹ nháº­n dáº¡ng giá»ng nÃ³i.

MÃ´ hÃ¬nh Markov áº©n Ä‘Æ°á»£c káº¿t há»£p bá»Ÿi 5 thÃ nh pháº§n, ta cÃ³ thá»ƒ gá»i má»™t mÃ´ hÃ¬nh Markov áº©n lÃ  $\lambda =(Q, V, A, B, \pi)$ (cÃ³ thá»ƒ Ä‘Æ¡n giáº£n hÃ³a kÃ½ hiá»‡u thÃ nh $\lambda =(A, B, \pi)$), trong Ä‘Ã³:

- $Q=q_1, q_2, \cdots, q_N$ lÃ  táº­p gá»“m $N$ tráº¡ng thÃ¡i áº©n, $X_t \in Q$ lÃ  giÃ¡ trá»‹ á»Ÿ thá»i Ä‘iá»ƒm $t$ Ä‘Æ°á»£c láº¥y trong táº­p $Q$.
- $O=o_1, o_2, \cdots, o_T$ lÃ  má»™t chuá»—i gá»“m $T$ (lÃ  thá»i Ä‘iá»ƒm cuá»‘i cÃ¹ng) quan sÃ¡t, má»—i quan sÃ¡t Ä‘Æ°á»£c láº¥y tá»« táº­p giÃ¡ trá»‹ duy nháº¥t $V = \\{v_1, v_2, \cdots, v_V\\}$. 
- $A_{N\times N}$ lÃ  ma tráº­n xÃ¡c suáº¥t chuyá»ƒn, Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  $A=a_{ij}=\\{P(X_{t+1} = q_j|X_t = q_i)|1 \le i,j \le N\\}$.
 á» Ä‘Ã¢y, $a_{ij}$ Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t chuyá»ƒn tá»« tráº¡ng thÃ¡i $i$ á»Ÿ thá»i Ä‘iá»ƒm $t$ sang tráº¡ng thÃ¡i $j$ á»Ÿ thá»i Ä‘iá»ƒm $t+1$
- $B_{V\times N}$ lÃ  ma tráº­n xÃ¡c suáº¥t phÃ¡t xáº¡ (emission probability), vÃ  Ä‘Æ°á»£c kÃ½ hiá»‡u bá»Ÿi $B=b_i(k)=\\{P(O_t = v_k|X_t=Q_i)|1\le i\le N, 1 \le k \le V\\}$.
 $b_i(k)$ Ä‘áº¡i diá»‡n cho xÃ¡c suáº¥t kÃ½ hiá»‡u $v_k$ Ä‘Æ°á»£c phÃ¡t xáº¡ ra tá»« tráº¡ng thÃ¡i $i$ táº¡i thá»i Ä‘iá»ƒm $t$.
- $\pi=\pi_i=\\{P(X_1=S_i)|1\le i \le n\\}$ 
lÃ  táº­p xÃ¡c suáº¥t khá»Ÿi táº¡o tráº¡ng thÃ¡i.

HÃ¬nh 2 mÃ´ táº£ trá»«u tÆ°á»£ng cáº¥u trÃºc cá»§a mÃ´ hÃ¬nh Markov áº©n Ä‘Æ°á»£c Ä‘á» cáº­p á»Ÿ trÃªn. HÃ¬nh $(a)$ lÃ  sÆ¡ Ä‘á»“ gá»“m cÃ¡c táº­p tráº¡ng thÃ¡i áº©n $q_i$ vÃ  cÃ¡c giÃ¡ trá»‹ xÃ¡c suáº¥t chuyá»ƒn $a_{ij}$, trÃ´ng giá»‘ng há»‡t nhÆ° má»™t Markov chain. HÃ¬nh $(b)$ mÃ´ táº£: vá»›i má»—i tráº¡ng thÃ¡i áº©n $q_i$, sáº½ cÃ³ má»™t táº­p giÃ¡ trá»‹ $v_k$ lÃ  táº­p giÃ¡ trá»‹ sáº½ Ä‘Æ°á»£c xuáº¥t ra vá»›i xÃ¡c suáº¥t $b_i(k)$ tÆ°Æ¡ng á»©ng, vÃ  hÃ¬nh $(b)$ lÃ  Ä‘iá»u khiáº¿n mÃ´ hÃ¬nh Markov áº©n khÃ¡c vá»›i Markov chain.

<p>
    <img src="/assets/hmm/hmm_abstract.svg" alt="hmm_abstract"/>
    <em>HÃ¬nh 1: Dáº¡ng vÃ  cáº¥u táº¡o cá»§a mÃ´ hÃ¬nh Markov áº©n trá»«u tÆ°á»£ng</em>
</p>

HÃ¬nh dÆ°á»›i Ä‘Ã¢y mÃ´ táº£ má»™t dáº¡ng hiá»‡n thá»±c cá»§a mÃ´ hÃ¬nh Markov áº©n, vá»›i $X_t \in Q$ vÃ  $O_t \in V$. Ta cÃ³ má»™t dáº¡ng cá»§a mÃ´ hÃ¬nh Markov áº©n theo thá»i gian thá»±c. Dáº¡ng $\cdots$ (ba cháº¥m) á»Ÿ Ä‘Ã¢y biá»ƒu thá»‹ cÃ¡c tráº¡ng thÃ¡i trÆ°á»›c Ä‘Ã³ vÃ  tráº¡ng thÃ¡i tÆ°Æ¡ng lai cÃ¡ch thá»i Ä‘iá»ƒm $t$ hÆ¡n $1$ Ä‘Æ¡n vá»‹. Theo tÃ­nh cháº¥t Markov, tráº¡ng thÃ¡i hiá»‡n táº¡i chá»‰ phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i tá»« quÃ¡ khá»© cÃ¡ch nÃ³ má»™t Ä‘Æ¡n vá»‹, vÃ  tráº¡ng thÃ¡i tÆ°Æ¡ng lai cÅ©ng chá»‰ phá»¥ thuá»™c vÃ o tráº¡ng thÃ¡i hiá»‡n táº¡i. Theo cÃ´ng thá»©c toÃ¡n há»c cÃ³ thá»ƒ mÃ´ táº£ lÃ :
$$P(X_t|X_{t-1}, X_{t-2}, X_{t-3}, \cdots)=P(X_t|X_{t-1})$$.

<p>
    <img src="/assets/hmm/hmm_realisation.svg" alt="hmm_realisation" />
    <em>HÃ¬nh 2: Má»™t dáº¡ng hiá»‡n thá»±c cá»§a mÃ´ hÃ¬nh Markov áº©n</em>
</p>

Do cÃ³ cáº¥u táº¡o nhÆ° hÃ¬nh 2, mÃ´ hÃ¬nh Markov áº©n ráº¥t thÃ­ch há»£p trong nhá»¯ng bÃ i toÃ¡n mÃ´ hÃ¬nh hÃ³a chuá»—i cÃ¡c giÃ¡ trá»‹. Trong thá»±c táº¿, ta cÃ³ thá»ƒ xem cÃ¡c chuá»—i giÃ¡ trá»‹ lÃ  dá»¯ liá»‡u chÃºng ta cÃ³ Ä‘Æ°á»£c tá»« thá»±c táº¿ vÃ  phÃ¢n phá»‘i Ä‘á»ƒ láº¥y ra chuá»—i giÃ¡ trá»‹ kia ta khÃ´ng há» biáº¿t trÆ°á»›c. Trong trÆ°á»ng há»£p nÃ y, ta cÃ³ thá»ƒ dÃ¹ng mÃ´ hÃ¬nh Markov áº©n Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a chuá»—i giÃ¡ trá»‹ Ä‘Ã³ Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c táº­p cÃ¡c tráº¡ng thÃ¡i áº©n vÃ  phÃ¢n phá»‘i xÃ¡c suáº¥t thÃ­ch há»£p, cÃ¡ch Ä‘á»ƒ há»c vÃ  láº¥y ra cÃ¡c tráº¡ng thÃ¡i áº©n sáº½ Ä‘Æ°á»£c trÃ¬nh bÃ y trong pháº§n 2.

Pháº§n tiáº¿p theo, pháº§n 2 sáº½ giá»›i thiá»‡u ba bÃ i toÃ¡n ná»n táº£ng cá»§a mÃ´ hÃ¬nh Markov áº©n, tuy ná»n táº£ng nhÆ°ng lÃ  ná»n mÃ³ng cho má»i bÃ i toÃ¡n phá»©c táº¡p hÆ¡n trong tháº¿ giá»›i thá»±c chiáº¿n.

<a name="-three-problems"></a>
# 2. Ba bÃ i toÃ¡n ná»n táº£ng

á» pháº§n 1, tÃ´i Ä‘Ã£ Ä‘i sÆ¡ lÆ°á»£c vá» cáº¥u táº¡o, cáº¥u trÃºc vÃ  cÃ¡c thÃ nh pháº§n Ä‘áº±ng sau mÃ´ hÃ¬nh Markov áº©n. Äáº¿n thá»i Ä‘iá»ƒm nÃ y, cháº¯c háº³n báº¡n Ä‘á»c sáº½ tháº¯c máº¯c cÃ¡ch sá»­ dá»¥ng mÃ´ hÃ¬nh Markov áº©n nhÆ° tháº¿ nÃ o, vÃ¬ thá»±c táº¿, mÃ´ hÃ¬nh Markov áº©n cÃ³ má»™t cáº¥u trÃºc dáº¡ng chuá»—i tuáº§n tá»± Ä‘áº·c biá»‡t, trÃ´ng ráº¥t khÃ¡c so vá»›i cÃ¡c mÃ´ hÃ¬nh truyá»n thá»‘ng nhÆ° Linear Regression, Logistic Regression, Random Forest, ... .

VÃ¬ tháº¿, mÃ´ hÃ¬nh Markov áº©n cÅ©ng sáº½ cÃ³ nhá»¯ng cÃ¡ch sá»­ dá»¥ng khÃ¡c. Cá»¥ thá»ƒ hÆ¡n, Ä‘á»ƒ sá»­ dá»¥ng mÃ´ hÃ¬nh Markov áº©n, ta báº¯t buá»™c pháº£i giáº£i quyáº¿t Ä‘Æ°á»£c 3 bÃ i toÃ¡n Ä‘Æ°á»£c mÃ´ táº£ dÆ°á»›i Ä‘Ã¢y. Ba bÃ i toÃ¡n Ä‘Ã³ lÃ :

- **BÃ i toÃ¡n 1**: ÄÆ°a trÆ°á»›c chuá»—i quan sÃ¡t $O=o_1, o_2, \cdots, o_T$ vÃ  mÃ´ hÃ¬nh $\lambda = (A, B, \pi)$. LÃ m cÃ¡ch nÃ o Ä‘á»ƒ ta cÃ³ thá»ƒ tÃ­nh hiá»‡u quáº£
$P(O|\lambda)$, chÃ­nh lÃ  xÃ¡c suáº¥t Ä‘á»ƒ chuá»—i quan sÃ¡t xáº£y ra khi biáº¿t trÆ°á»›c mÃ´ hÃ¬nh?
- **BÃ i toÃ¡n 2**: ÄÆ°a trÆ°á»›c chuá»—i quan sÃ¡t $O=o_1, o_2, \cdots, o_T$ vÃ  mÃ´ hÃ¬nh $\lambda = (A, B, \pi)$. LÃ m cÃ¡ch nÃ o Ä‘á»ƒ ta cÃ³ thá»ƒ tÃ¬m Ä‘Æ°á»£c má»™t chuá»—i tráº¡ng thÃ¡i áº©n $X=X_1, X_2, \cdots, X_T$ Ä‘á»ƒ giáº£i thÃ­ch tá»‘t nháº¥t cho chuá»—i quan sÃ¡t $O$?
- **BÃ i toÃ¡n 3**: LÃ m cÃ¡ch nÃ o Ä‘á»ƒ ta cÃ³ thá»ƒ Ä‘iá»u chá»‰nh tham sá»‘ cá»§a mÃ´ hÃ¬nh $\lambda = (A, B, \pi)$ Ä‘á»ƒ tá»‘i Ä‘a hÃ³a xÃ¡c suáº¥t 
$P(O|\lambda)$?

BÃ i toÃ¡n 1 lÃ  bÃ i toÃ¡n Ä‘Ã¡nh giÃ¡ (evaluation problem), nghÄ©a lÃ  Ä‘i tÃ­nh xÃ¡c suáº¥t xáº£y ra cá»§a má»™t chuá»—i quan sÃ¡t khi ta cÃ³ Ä‘Æ°á»£c mÃ´ hÃ¬nh. Náº¿u nhÃ¬n á»Ÿ má»™t khÃ­a cáº¡nh khÃ¡c, Ä‘Ã¢y chÃ­nh lÃ  bÃ i toÃ¡n cháº¥m Ä‘iá»ƒm mÃ´ hÃ¬nh, náº¿u mÃ´ hÃ¬nh nÃ o cÃ³ xÃ¡c suáº¥t 
$P(O|\lambda)$ cao hÆ¡n nghÄ©a lÃ  mÃ´ hÃ¬nh Ä‘Ã³ tá»‘t hÆ¡n. BÃ i toÃ¡n 2 lÃ  bÃ i toÃ¡n giáº£i mÃ£ (decoding problem), cÃ³ thá»ƒ hiá»ƒu lÃ  ta Ä‘Ã£ cÃ³ má»™t chuá»—i quan sÃ¡t $O$ vÃ  ta cÃ³ thá»ƒ tháº¥y, bÃ¢y giá» ta pháº£i tÃ¬m má»™t chuá»—i tráº¡ng thÃ¡i áº©n tÆ°Æ¡ng á»©ng (cÃ³ cÃ¹ng kÃ­ch cá»¡) $X$ sao cho giáº£i thÃ­ch tá»‘t nháº¥t chuá»—i quan sÃ¡t $O$ kia. BÃ i toÃ¡n 3 lÃ  bÃ i toÃ¡n há»c (learning problem), lÃ  bÃ i toÃ¡n quan trá»ng nháº¥t. VÃ¬ nhá» bÃ i toÃ¡n 3, ta cÃ³ thá»ƒ tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh Markov áº©n $\lambda$ Ä‘áº¿n má»©c há»™i tá»¥, sá»­ dá»¥ng cho nhiá»u bÃ i toÃ¡n thá»±c táº¿ khÃ¡c nhau.

Cáº£ ba bÃ i toÃ¡n trÃªn Ä‘á»u cÃ³ cÃ¡ch giáº£i ráº¥t Ä‘Æ¡n giáº£n, Ä‘Ã³ lÃ  tháº¿ vÃ o vÃ  thá»­, tuy nhiÃªn Ä‘á»™ phá»©c táº¡p tÃ­nh toÃ¡n sáº½ ráº¥t cao, nÃªn ngÆ°á»i ta dÃ¹ng ká»¹ thuáº­t quy hoáº¡ch Ä‘á»™ng (<a href="https://en.wikipedia.org/wiki/Dynamic_programming" target="_blank">dynamic programming</a>) Ä‘á»ƒ tá»‘i Æ°u, giÃºp giáº£i quyáº¿t cáº£ 3 váº¥n Ä‘á» má»™t cÃ¡ch quy náº¡p vÃ  theo Ä‘á»™ phá»©c táº¡p Ä‘a thá»©c. Cá»¥ thá»ƒ bÃ i toÃ¡n 1 cÃ³ thá»ƒ giáº£i vá»›i thuáº­t toÃ¡n <a href="https://en.wikipedia.org/wiki/Forward%E2%80%93backward_algorithm" target="_blank">forward-backward</a>, bÃ i toÃ¡n 2 sáº½ giáº£i báº±ng thuáº­t toÃ¡n <a href="https://en.wikipedia.org/wiki/Viterbi_algorithm" target="_blank">Viterbi</a> vÃ  bÃ i toÃ¡n 3 sáº½ giáº£i báº±ng thuáº­t toÃ¡n <a href="https://en.wikipedia.org/wiki/Baum%E2%80%93Welch_algorithm" target="_blank">Baum-Welch</a>.

Trong pháº§n nÃ y, tÃ´i chá»‰ Ä‘i giá»›i thiá»‡u vá» ba bÃ i toÃ¡n, vá» cÃ¡ch giáº£i sáº½ khÃ´ng Ä‘Æ°á»£c Ä‘á» cáº­p Ä‘áº¿n, báº¡n Ä‘á»c cÃ³ há»©ng thÃº vá»›i lá»i giáº£i cho ba bÃ i toÃ¡n cÃ³ thá»ƒ tham kháº£o [[3]](#-reference-3), Ä‘Ã¢y lÃ  bÃ i bÃ¡o ráº¥t cháº¥t lÆ°á»£ng vá» mÃ´ hÃ¬nh Markov áº©n, lÃ  ná»n táº£ng cho báº¥t cá»© ai má»›i báº¯t Ä‘áº§u tÃ¬m hiá»ƒu vá» mÃ´ hÃ¬nh Markov áº©n. Náº¿u gáº·p khÃ³ khÄƒn trong viá»‡c hiá»‡n thá»±c thuáº­t toÃ¡n, báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o Ä‘áº¿n <a href="https://github.com/tuanio/hmm" target="_blank">github</a> cá»§a tÃ´i, tÃ´i cÅ©ng Ä‘Ã£ Ä‘á»c bÃ i bÃ¡o sá»‘ [[3]](#-reference-3) vÃ  hiá»‡n thá»±c thÃ nh cÃ´ng.

<a name="-bai-toan-phan-tich-cam-xuc-van-ban"></a>
# 3. BÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n

<a name="-gioi-thieu-bai-toan"></a>
## 3.1 Giá»›i thiá»‡u bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n
PhÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n (<a href="https://en.wikipedia.org/wiki/Sentiment_analysis" target="_blank">sentiment analysis</a>) lÃ  bÃ i toÃ¡n Ä‘Æ°á»£c nghiÃªn cá»©u trong lÄ©nh vá»±c Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn. Má»¥c tiÃªu cá»§a bÃ i toÃ¡n lÃ  tÃ¬m ra cáº£m xÃºc (*tÃ­ch cá»±c*, *tiÃªu cá»±c*, *trung tÃ­nh*) cá»§a má»™t cÃ¢u chá»¯ trong má»™t lÄ©nh vá»±c cá»¥ thá»ƒ nÃ o Ä‘Ã³. BÃ i toÃ¡n nÃ y ráº¥t Ä‘Æ°á»£c Æ°a chuá»™ng trong cÃ¡c cÃ´ng ty mÃ  lÆ°á»£ng dá»¯ liá»‡u vá» chá»¯ cá»§a há» lá»›n, há» cÃ³ thá»ƒ khai thÃ¡c thÃ´ng tin tá»« nguá»“n dá»¯ liá»‡u cá»§a há», tá»« Ä‘Ã³ hiá»ƒu Ä‘Æ°á»£c khÃ¡ch hÃ ng cá»§a há» cáº§n gÃ¬. VÃ­ dá»¥ nhÆ° cÃ¡c bÃ¬nh luáº­n trÃªn shopee hay tiki lÃ  má»™t vÃ­ dá»¥, má»™t cÃ¢u "TÃ´i ráº¥t thÃ­ch sáº£n pháº©m nÃ y" sáº½ Ä‘Æ°á»£c Ä‘Ã¡nh nhÃ£n lÃ  *tÃ­ch cá»±c*, cÃ¢u "Sáº£n pháº©m nÃ y nhÄƒn nheo quÃ¡" sáº½ Ä‘Æ°á»£c gÃ¡n nhÃ£n lÃ  *tiÃªu cá»±c*, má»™t trÆ°á»ng há»£p khÃ¡c cÃ³ nhÃ£n lÃ  *trung tÃ­nh*, khÃ´ng rÃµ rÃ ng *tÃ­ch cá»±c* hay *tiÃªu cá»±c*, vÃ­ dá»¥ nhÆ° cÃ¢u "HÃ´m nay tÃ´i vá»«a nháº­n Ä‘Æ°á»£c sáº£n pháº©m nÃ y".

Hiá»‡n táº¡i, bÃ i toÃ¡n nÃ y cÃ³ thá»ƒ giáº£i quyáº¿t báº±ng nhá»¯ng phÆ°Æ¡ng phÃ¡p Machine Learning hoáº·c máº¡nh hÆ¡n lÃ  Deep Learning, chi tiáº¿t báº¡n Ä‘á»c cÃ³ thá»ƒ tÃ¬m hiá»ƒu á»Ÿ <a href="https://paperswithcode.com/task/sentiment-analysis" target="_blank">Ä‘Ã¢y</a>. NhÆ°ng trong pháº¡m vi bÃ i viáº¿t nÃ y, chÃºng ta sáº½ tiáº¿p cáº­n vá»›i má»™t hÆ°á»›ng khÃ¡c, Ä‘Ã³ lÃ  giáº£i quyáº¿t bÃ i toÃ¡n nÃ y báº±ng mÃ´ hÃ¬nh Markov áº©n.

<a name="-bo-du-lieu"></a>
## 3.2 Bá»™ dá»¯ liá»‡u Financial News cá»§a Kaggle

Bá»™ dá»¯ liá»‡u chÃºng ta sáº½ Ä‘i nghiÃªn cá»©u lÃ  bá»™ <a href="https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news" target="_blank">Financial News</a> Ä‘Æ°á»£c láº¥y trÃªn Kaggle. Dá»¯ liá»‡u gá»“m 2 cá»™t, 4837 hÃ ng, cá»™t thá»© nháº¥t lÃ  nhÃ£n, tá»©c lÃ  cáº£m xÃºc cá»§a vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c gáº¯n tá»« trÆ°á»›c, gá»“m 3 giÃ¡ trá»‹: `positive`, `neutral`, `negative`. Cá»™t thá»© hai lÃ  vÄƒn báº£n. Dá»¯ liá»‡u nÃ y Ä‘áº§y Ä‘á»§ vÃ  Ä‘Æ¡n giáº£n Ä‘á»ƒ sá»­ dá»¥ng trong bÃ i toÃ¡n nÃ y.

<a name="-mo-hinh-bai-toan"></a>
## 3.3 MÃ´ hÃ¬nh bÃ i toÃ¡n

Khi á»©ng dá»¥ng mÃ´ hÃ¬nh Markov áº©n vÃ o bÃ i toÃ¡n phÃ¢n lá»›p (classification) nhÆ° chÃºng ta Ä‘ang Ä‘á»‹nh lÃ m, chÃºng ta pháº£i mÃ´ hÃ¬nh hÃ³a má»™t sá»‘ lÆ°á»£ng mÃ´ hÃ¬nh Markov áº©n riÃªng biá»‡t báº±ng vá»›i sá»‘ lÆ°á»£ng lá»›p cá»§a bÃ i toÃ¡n. Náº¿u láº¥y bá»™ dá»¯ liá»‡u Financial News kia lÃ m chuáº©n, ta sáº½ cÃ³ 3 mÃ´ hÃ¬nh Markov áº©n tÆ°Æ¡ng á»©ng vá»›i 3 lá»›p `positive`, `neutral` vÃ  `negative`.

MÃ´ hÃ¬nh Markov áº©n sáº½ lÃ m tá»‘t cÃ´ng viá»‡c cá»§a mÃ¬nh trong viá»‡c mÃ´ hÃ¬nh hÃ³a phÃ¢n phá»‘i xÃ¡c suáº¥t cá»§a riÃªng tá»«ng lá»›p. Náº¿u coi táº­p dá»¯ liá»‡u lÃ  $O$ vÃ  cÃ³ tá»•ng cá»™ng 3 mÃ´ hÃ¬nh Markov áº©n tÆ°Æ¡ng á»©ng vá»›i 3 lá»›p thÃ¬ lá»›p dá»± Ä‘oÃ¡n khi ta Ä‘Æ°a dá»¯ liá»‡u má»›i vÃ o sáº½ theo cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y:

$$C^\star = \underset{C}{\mathrm{argmax }} P(O|\lambda_C)$$

Trong Ä‘Ã³:
- $C$ lÃ  lá»›p (nhÃ£n) vÃ  $C^\star$ lÃ  lá»›p dá»± Ä‘oÃ¡n.
- $\lambda_C$ lÃ  mÃ´ hÃ¬nh Markov áº©n tÆ°Æ¡ng á»©ng vá»›i má»—i lá»›p.
- $P(O|\lambda_C)$
chÃ­nh lÃ  bÃ i toÃ¡n 1, bÃ i toÃ¡n Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh.

Báº¥t cá»© mÃ´ hÃ¬nh Machine Learning nÃ o cÅ©ng sáº½ cÃ³ giai Ä‘oáº¡n huáº¥n luyá»‡n, mÃ´ hÃ¬nh Markov áº©n cÅ©ng khÃ´ng ngoáº¡i lá»‡. HÃ¬nh (3a) mÃ´ táº£ quy trÃ¬nh nÃ y, ban Ä‘áº§u ta cÃ³ má»™t táº­p dá»¯ liá»‡u $O$ (cÃ³ thá»ƒ lÃ  nhiá»u $O$) vÃ  $n$ lá»›p (nhÃ£n) tÆ°Æ¡ng á»©ng. Ta sáº½ chia táº­p dá»¯ liá»‡u ra thÃ nh $n$ táº­p dá»¯ liá»‡u nhá» hÆ¡n tÆ°Æ¡ng á»©ng vá»›i $n$ nhÃ£n. Sau Ä‘Ã³ dÃ¹ng thuáº­t toÃ¡n Baum-Welch (bÃ i toÃ¡n sá»‘ 3) Ä‘á»ƒ huáº¥n luyá»‡n cho mÃ´ hÃ¬nh $\lambda_{C_i}$ tÆ°Æ¡ng á»©ng. Káº¿t thÃºc quÃ¡ trÃ¬nh huáº¥n luyá»‡n, ta Ä‘Æ°á»£c $n$ mÃ´ hÃ¬nh Markov áº©n tÆ°Æ¡ng á»©ng vá»›i $n$ nhÃ£n lá»›p.

Äá»ƒ cÃ³ thá»ƒ sá»­ dá»¥ng $n$ mÃ´ hÃ¬nh kia trong quÃ¡ trÃ¬nh kiá»ƒm thá»­ hoáº·c Ä‘i dá»± Ä‘oÃ¡n. Ta cáº§n Ä‘Æ°a dá»¯ liá»‡u kiá»ƒm thá»­ cho cáº£ $n$ mÃ´ hÃ¬nh Markov áº©n, sau Ä‘Ã³ Ä‘i tÃ¬m cÃ¡c xÃ¡c suáº¥t 
$P(O|\lambda_{C_i})$ (bÃ i toÃ¡n sá»‘ 1) vÃ  chá»n nhÃ£n $C$ cÃ³ giÃ¡ trá»‹ xÃ¡c suáº¥t lá»›n nháº¥t, nhÃ£n $C$ nÃ y sáº½ lÃ  nhÃ£n dá»± Ä‘oÃ¡n cho chuá»—i quan sÃ¡t $O$ ta Ä‘Æ°a vÃ o. HÃ¬nh (3b) mÃ´ táº£ rÃµ quy trÃ¬nh nÃ y.

<p>
    <img src="/assets/hmm/hmm_diagram.svg" alt="hmm_diagram" />
    <em>HÃ¬nh 3: SÆ¡ Ä‘á»“ cá»§a mÃ´ hÃ¬nh Markov áº©n (a) trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  (b) trong quÃ¡ trÃ¬nh kiá»ƒm thá»­</em>
</p>

<a name="-phuong-phap"></a>
## 3.4 PhÆ°Æ¡ng phÃ¡p thá»±c hiá»‡n

BÃ¢y giá» chÃºng ta sáº½ Ä‘i Ä‘áº¿n pháº§n hiá»‡n thá»±c bÃ i toÃ¡n, tÃ´i sáº½ sá»­ dá»¥ng ngÃ´n ngá»¯ láº­p trÃ¬nh Python vá»›i cÃ¡c thÆ° viá»‡n á»Ÿ Ã´ code dÆ°á»›i Ä‘Ã¢y.

âš ï¸ **LÆ°u Ã½**: khi code bÃ¡o lá»—i thÆ° viá»‡n, cÃ¡c báº¡n cÃ³ thá»ƒ tá»± cÃ i thÆ° viá»‡n thÃ´ng qua `pip install {tÃªn thÆ° viá»‡n}`.

````python
import numpy as np # thÆ° viá»‡n tÃ­nh toÃ¡n 
import pandas as pd # Ä‘á»c file csv
import concurrent.futures as cf # thÆ° viá»‡n giÃºp code python cháº¡y Ä‘a luá»“ng
from hmmlearn import hmm # thÆ° viá»‡n mÃ´ hÃ¬nh Markov áº©n 
from sklearn.cluster import KMeans # lÆ°á»£ng hÃ³a vector
from sklearn.metrics import accuracy_score # Ä‘o Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh
from sklearn.decomposition import TruncatedSVD # giáº£m chiá»u dá»¯ liá»‡u
from sklearn.model_selection import train_test_split # chia táº­p dá»¯ liá»‡u train|test
from sklearn.feature_extraction.text import TfidfVectorizer # táº¡o feature cho mÃ´ hÃ¬nh tá»« chá»¯
````

ğŸ‘‰ ChÃºng ta sáº½ Ä‘i qua cÃ¡c bÆ°á»›c nhÆ° sau:
1. Táº¡o feature dá»¯ liá»‡u sá»‘ tá»« dá»¯ liá»‡u chá»¯ cÃ³ sáºµn báº±ng TF-IDF.
2. LÆ°á»£ng hÃ³a vector (vector quantization) dá»¯ liá»‡u sá»‘ liÃªn tá»¥c thÃ nh dáº¡ng Ä‘á»‹nh tÃ­nh cÃ³ thá»ƒ Ä‘em Ä‘i huáº¥n luyá»‡n.
3. Chia táº­p dá»¯ liá»‡u huáº¥n luyá»‡n, kiá»ƒm thá»­ tÆ°Æ¡ng á»©ng.
4. Huáº¥n luyá»‡n bá»™ mÃ´ hÃ¬nh Markov áº©n vá»›i táº­p dá»¯ liá»‡u huáº¥n luyá»‡n.
5. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh Markov áº©n thÃ´ng qua táº­p dá»¯ liá»‡u kiá»ƒm thá»­.

TrÆ°á»›c tiÃªn, ta sáº½ Ä‘á»c dá»¯ liá»‡u Ä‘á»ƒ cÃ³ thá»ƒ chuáº©n bá»‹ cho bÆ°á»›c táº¡o feature cho mÃ´ hÃ¬nh. Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» cáº­p trong pháº§n 3.2.

````python
df = pd.read_csv('all-data.csv', encoding="ISO-8859-1", header=None, names=['label', 'text'])
````

Äá»ƒ tráº¡ng thÃ¡i cá»§a code khÃ´ng thay Ä‘á»•i qua má»—i láº§n cháº¡y, ta nÃªn gÃ¡n cá»¥ thá»ƒ giÃ¡ trá»‹ `random state` cho cÃ¡c thÆ° viá»‡n. DÆ°á»›i Ä‘Ã¢y tÃ´i Ä‘á»‹nh nghÄ©a biáº¿n `rs` lÃ  giÃ¡ trá»‹ `random state` Ä‘á»ƒ dÃ¹ng cho cÃ¡c code sau. GiÃ¡ trá»‹ cÃ¡c báº¡n cÃ³ thá»ƒ thay Ä‘á»•i báº¥t ká»³.

````python
rs = 8
````

Táº¡o biáº¿n `corpus` Ä‘á»ƒ gÃ¡n dá»¯ liá»‡u chá»¯ vÃ o, tiá»‡n sá»­ dá»¥ng vá» sau.

````python
corpus = df['text'].values
````

NhÆ° cÃ¡c mÃ´ hÃ¬nh Machine Learning truyá»n thá»‘ng khÃ¡c, mÃ´ hÃ¬nh Markov áº©n sáº½ chá»‰ lÃ m viá»‡c Ä‘Æ°á»£c vá»›i cÃ¡c giÃ¡ trá»‹ sá»‘. MÃ  dá»¯ liá»‡u ban Ä‘áº§u cá»§a chÃºng ta lÃ  dá»¯ liá»‡u dáº¡ng chá»¯, nÃªn ta pháº£i chuyá»ƒn tá»« chá»¯ sang sá»‘. Äá»ƒ lÃ m nhÆ° váº­y, ta sá»­ dá»¥ng TF-IDF Ä‘á»ƒ tÃ­nh toÃ¡n cÃ¡c giÃ¡ trá»‹ trá»ng sá»‘ Ä‘á»ƒ Ä‘áº¡i diá»‡n cho tá»«ng tá»« má»™t trong bá»™ ngá»¯ liá»‡u ban Ä‘áº§u. Chi tiáº¿t hÆ¡n vá» TF-IDF, báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o á»Ÿ <a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" target="_blank">Ä‘Ã¢y</a>. CÃ²n trong Python, ta sáº½ tÃ­nh báº±ng Ä‘oáº¡n code sau:

````python
tfidf = TfidfVectorizer(stop_words='english')
transformed = tfidf.fit_transform(corpus)

print("KÃ­ch cá»¡ dá»¯ liá»‡u:", transformed.shape)
````
````plain
KÃ­ch cá»¡ dá»¯ liá»‡u: (4846, 9820)
````

NhÆ° báº¡n Ä‘á»c cÅ©ng Ä‘Ã£ tháº¥y, cÃ³ táº­n 9820 cá»™t dá»¯ liá»‡u Ä‘Æ°á»£c táº¡o ra, nhÆ° váº­y lÃ  quÃ¡ nhiá»u, ta pháº£i dÃ¹ng cÃ¡ch nÃ o Ä‘Ã³ Ä‘á»ƒ giá»¯ láº¡i cÃ¡c thÃ´ng tin quan trá»ng nháº¥t, giáº£m bá»›t sá»‘ lÆ°á»£ng cá»™t láº¡i, nhá» Ä‘Ã³ giÃºp giáº£m thá»i gian huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­, mÃ´ hÃ¬nh cÅ©ng khÃ´ng pháº£i há»c nhá»¯ng thÃ´ng tin dÆ° thá»«a. Trong trÆ°á»ng há»£p nÃ y, ta sáº½ dÃ¹ng `Truncated SVD` vá»›i sá»‘ lÆ°á»£ng cá»™t ta muá»‘n giá»¯ láº¡i lÃ  300. Chi tiáº¿t vá» `Truncated SVD`, báº¡n Ä‘á»c cÃ³ thá»ƒ tham kháº£o á»Ÿ <a href="https://machinelearningcoban.com/2017/06/07/svd/#-truncated-svd" target="_blank">blog machine learning cÆ¡ báº£n</a>.

````python
svd = TruncatedSVD(n_components=300, random_state=rs)
X_transformed = svd.fit_transform(transformed)

print("KÃ­ch cá»¡ dá»¯ liá»‡u:", X_transformed.shape)
print(X_transformed)
````
````plain
KÃ­ch cá»¡ dá»¯ liá»‡u: (4846, 300)
[[ 2.74853772e-02  1.23546023e-01 -9.18267054e-02 ... -1.04585446e-02
   6.31456379e-02  1.64352977e-02]
 [ 1.71779475e-02  6.60849287e-02 -3.16730100e-02 ...  3.16069539e-02
   2.32036018e-02 -8.03645790e-03]
 [ 2.53099565e-02  8.71819162e-02 -5.00550792e-02 ... -5.08493999e-02
  -6.52592321e-02 -4.16690704e-02]
 ...
 [ 6.29146372e-01 -1.92710754e-01  3.24730118e-02 ...  3.28086253e-02
  -1.07719957e-02 -8.53337727e-04]
 [ 6.66497950e-01 -1.41546038e-01  1.81541966e-03 ...  7.96125651e-03
  -2.97791037e-03 -1.60182069e-04]
 [ 9.54134895e-02  1.71111951e-01 -6.08651229e-02 ... -1.30032385e-02
   3.79181183e-02  1.24508153e-02]]
````

NgoÃ i váº¥n Ä‘á» cÃ³ quÃ¡ nhiá»u cá»™t trong feature Ä‘Ã£ Ä‘Æ°á»£c giáº£i quyáº¿t, ta cÃ²n gáº·p thÃªm má»™t váº¥n Ä‘á» ná»¯a Ä‘Ã³ lÃ  dá»¯ liá»‡u khÃ´ng phÃ¹ há»£p vá»›i mÃ´ hÃ¬nh Markov áº©n. NhÆ° Ä‘Ã£ tÃ¬m hiá»ƒu trÃªn pháº§n 1, cÃ¡c quan sÃ¡t $O$ cá»§a mÃ´ hÃ¬nh Markov áº©n Ä‘Æ°á»£c láº¥y tá»« má»™t táº­p $V$ pháº§n tá»­, vÃ¬ tháº¿, dá»¯ liá»‡u Ä‘Æ°a vÃ o cho mÃ´ hÃ¬nh Markov áº©n pháº£i lÃ  dáº¡ng Ä‘á»‹nh tÃ­nh.

Äá»ƒ giáº£i quyáº¿t váº¥n Ä‘á» trÃªn, ta cÃ³ thá»ƒ dÃ¹ng má»™t ká»¹ thuáº­t Ä‘Æ°á»£c gá»i lÃ  lÆ°á»£ng hÃ³a vector (<a href="https://en.wikipedia.org/wiki/Vector_quantization" target="_blank">vector quantization</a>). LÆ°á»£ng hÃ³a vector cÃ³ thá»ƒ hiá»ƒu Ä‘Æ¡n giáº£n lÃ  phÃ¢n cá»¥m cÃ¡c giÃ¡ trá»‹ liÃªn tá»¥c thÃ nh má»™t táº­p cÃ¡c cá»¥m cÃ³ sá»± giá»‘ng nhau. Chá»‰ sá»‘ cá»§a cÃ¡c cá»¥m bÃ¢y giá» cÃ³ thá»ƒ coi nhÆ° lÃ  cÃ¡c giÃ¡ trá»‹ Ä‘Æ°á»£c láº¥y trong táº­p $V = \text{sá»‘ cá»¥m}$ pháº§n tá»­. GiÃ¡ trá»‹ trong cÃ¹ng má»™t táº­p sáº½ cÃ³ cÃ¹ng má»™t chá»‰ sá»‘ nÃ y. á» pháº§n hiá»‡n thá»±c, tÃ´i sáº½ Ä‘i lÆ°á»£ng hÃ³a vector báº±ng thuáº­t toÃ¡n <a href="https://en.wikipedia.org/wiki/K-means_clustering" target="_blank">K-Means</a> vá»›i sá»‘ cá»¥m lÃ  30.

````python
X_cluster = X_transformed.reshape(-1, 1) 

vq = KMeans(n_clusters=30) # vector quantization
vq.fit(X_cluster)

def map_vq(x):
    return vq.predict(x.reshape(-1, 1))

with cf.ThreadPoolExecutor() as exe:
    X = np.array(list(exe.map(map_vq, X_transformed)))

print(X)
````
````plain
[[ 9 11  0 ...  8 16 22]
 [27 16 20 ...  9 27  8]
 [ 9  3 19 ... 19 28 10]
 ...
 [26 24  4 ...  4  8 13]
 [26 24 13 ...  1 29 13]
 [ 3 18 28 ... 17  4 22]]
````

NhÆ° báº¡n cÃ³ thá»ƒ tháº¥y, dá»¯ liá»‡u Ä‘á»‹nh lÆ°á»£ng Ä‘Æ°á»£c láº¥y ra tá»« TF-IDF Ä‘Ã£ chuyá»ƒn thÃ nh dáº¡ng sá»‘ nguyÃªn, lÃ  chá»‰ sá»‘ cá»§a cÃ¡c cá»¥m. BÃ¢y giá», ta sáº½ Ä‘i phÃ¢n chia dá»¯ liá»‡u thÃ nh hai táº­p: huáº¥n luyá»‡n vÃ  kiá»ƒm thá»­ vá»›i tá»‰ lá»‡ 8:2.

````python
y = df['label']

Xtrain, Xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=rs)
````

Sau khi Ä‘Ã£ cÃ³ dá»¯ liá»‡u, ta sáº½ Ä‘i táº¡o mÃ´ hÃ¬nh Markov áº©n cho bÃ i toÃ¡n nÃ y rá»“i má»›i huáº¥n luyá»‡n. Ta sáº½ táº¡o cáº¥u trÃºc mÃ´ hÃ¬nh giá»‘ng nhÆ° hÃ¬nh 3.

````python
class HMMSystem:
    def __init__(self, n_components=10, random_state=rs):
        self.n_components = n_components
        self.random_state = random_state
    
    def fit(self, X, y):
        self.labels = np.unique(y)
        self.X = X
        self.y = y
        self.hmm_models = {}
        for c in self.labels:
            with cf.ThreadPoolExecutor() as exe:
                self.hmm_models = list(exe.map(self._create_model, self.labels))
            self.hmm_models = dict(zip(self.labels, self.hmm_models))
        return self
        
    def predict(self, X):
        with cf.ThreadPoolExecutor() as exe:
            pred = np.array(list(exe.map(self._find_class, X)))
        return pred
    
    def _create_model(self, label):
        model = hmm.MultinomialHMM(
            n_components=self.n_components,
            random_state=self.random_state
        ).fit(self.X[self.y == label])
        return model
    
    def _find_class(self, data):
        def _decode(model):
            return model.decode([data])[0]

        with cf.ThreadPoolExecutor() as exe:
            logprobs = list(exe.map(_decode, self.hmm_models.values()))

        return self.labels[np.argmax(logprobs)]
````

Huáº¥n luyá»‡n mÃ´ hÃ¬nh vá»›i táº­p dá»¯ liá»‡u huáº¥n luyá»‡n. Ta sáº½ chá»n sá»‘ lÆ°á»£ng tráº¡ng thÃ¡i áº©n $Q$ lÃ  8.

âš ï¸ **LÆ°u Ã½:** quÃ¡ trÃ¬nh huáº¥n luyá»‡n cÃ³ thá»ƒ hÆ¡i lÃ¢u, khoáº£ng vÃ i phÃºt.

````python
model = HMMSystem(8, rs)
model.fit(X, y)
````

DÃ¹ng táº­p kiá»ƒm thá»­ Ä‘á»ƒ dá»± Ä‘oÃ¡n vá»›i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n.

````python
ytest_pred = model.predict(Xtest)
ytrain_pred = model.predict(Xtrain)
````

Cuá»‘i cÃ¹ng lÃ  Ä‘i Ä‘Ã¡nh giÃ¡ trÃªn cáº£ hai táº­p dá»¯ liá»‡u Ä‘Ã£ dá»± Ä‘oÃ¡n.

````python
acc_train = accuracy_score(ytrain, ytrain_pred)
acc_test = accuracy_score(ytest, ytest_pred)

print("Äá»™ chÃ­nh xÃ¡c táº­p huáº¥n luyá»‡n: %.2f/1" % acc_train)
print("Äá»™ chÃ­nh xÃ¡c táº­p kiá»ƒm thá»­: %.2f/1" % acc_test)
````
````plain
Äá»™ chÃ­nh xÃ¡c táº­p huáº¥n luyá»‡n: 0.46/1
Äá»™ chÃ­nh xÃ¡c táº­p kiá»ƒm thá»­: 0.48/1
````

Äá»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh lÃ  $48\%$ trÃªn táº­p kiá»ƒm thá»­, má»©c Ä‘á»™ chÃ­nh xÃ¡c nÃ y vÃ o khoáº£ng giá»¯a, 50:50, nÃªn káº¿t quáº£ dá»± Ä‘oÃ¡n cá»§a mÃ´ hÃ¬nh cÃ²n khÃ¡ rá»§i ro.

ToÃ n bá»™ code Python hiá»‡n thá»±c sáº½ Ä‘á»ƒ á»Ÿ <a href="https://github.com/tuanio/sentiment-analysis-discrete-hmm" target="_blank">Ä‘Ã¢y</a>.

<a name="-tong-ket"></a>
# 4. Tá»•ng káº¿t

Trong bÃ i viáº¿t nÃ y, chÃºng ta Ä‘Ã£ Ä‘i qua sÆ¡ lÆ°á»£c vá» Ä‘á»‹nh nghÄ©a, cÃ¡c thÃ nh pháº§n vÃ  cÃ¡c bÃ i toÃ¡n cá»§a mÃ´ hÃ¬nh Markov áº©n. Tá»« Ä‘Ã³ á»©ng dá»¥ng mÃ´ hÃ¬nh Markov áº©n vÃ o bÃ i toÃ¡n phÃ¢n tÃ­ch cáº£m xÃºc vÄƒn báº£n. Äá»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh khÃ´ng quÃ¡ cao, nhÆ°ng nÃ³ giÃºp báº¡n Ä‘á»c cÃ³ thá»ƒ hiá»ƒu thÃªm vá» mÃ´ hÃ¬nh nÃ y. Báº¡n Ä‘á»c cÃ³ thá»ƒ Ä‘á»c thÃªm vá» mÃ´ hÃ¬nh Markov áº©n á»Ÿ cÃ¡c bÃ i bÃ¡o vÃ  Ä‘á»‹a chá»‰ website trong pháº§n 5.

<a name="-tham-khao"></a>
# 5. Tham kháº£o


[1] https://web.stanford.edu/~jurafsky/slp3/A.pdf

[2] https://en.wikipedia.org/wiki/Hidden_Markov_model

[3] A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. Rabiner. 1989.
<a name="-reference-3" ></a>

[4] A Systematic Review of Hidden Markov Models and Their Applications. Bhavya Mor, Sunita Garhwal & Ajay Kumar. 2021.

[5] Hidden Markov Models for Sentiment Analysis in Social Medias. Isidoros Perikos. 2019.